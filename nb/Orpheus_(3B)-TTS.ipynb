{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ronaldbvirinyangwe/How-to-subscribe-to-Chikoro-AI/blob/main/nb/Orpheus_(3B)-TTS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EK5zGYcsiDrI"
      },
      "source": [
        "To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n",
        "<div class=\"align-center\">\n",
        "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
        "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + ⭐ <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ⭐\n",
        "</div>\n",
        "\n",
        "To install Unsloth on your own computer, follow the installation instructions on our Github page [here](https://docs.unsloth.ai/get-started/installing-+-updating).\n",
        "\n",
        "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWJbHkjKiDrN"
      },
      "source": [
        "### News"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loASddBtiDrP"
      },
      "source": [
        "**NEW** Unsloth now supports training the new **gpt-oss** model from OpenAI! You can start finetune gpt-oss for free with our **[Colab notebook](https://x.com/UnslothAI/status/1953896997867729075)**!\n",
        "\n",
        "Unsloth now supports Text-to-Speech (TTS) models. Read our [guide here](https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning).\n",
        "\n",
        "Read our **[Gemma 3N Guide](https://docs.unsloth.ai/basics/gemma-3n-how-to-run-and-fine-tune)** and check out our new **[Dynamic 2.0](https://docs.unsloth.ai/basics/unsloth-dynamic-2.0-ggufs)** quants which outperforms other quantization methods!\n",
        "\n",
        "Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h23TbNqpiDrR"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ENaJffq1iDrS"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "    !pip install --no-deps unsloth\n",
        "!pip install snac"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkWYsztAs9Ky"
      },
      "source": [
        "### Unsloth\n",
        "\n",
        "`FastModel` supports loading nearly any model now! This includes Vision and Text models!\n",
        "\n",
        "Thank you to [Etherl](https://huggingface.co/Etherll) for creating this notebook!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-22T00:48:54.511089Z",
          "iopub.status.busy": "2025-03-22T00:48:54.510770Z",
          "iopub.status.idle": "2025-03-22T00:51:37.363415Z",
          "shell.execute_reply": "2025-03-22T00:51:37.362696Z",
          "shell.execute_reply.started": "2025-03-22T00:48:54.511053Z"
        },
        "id": "QmUBVEnvCDJv",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "fourbit_models = [\n",
        "    # 4bit dynamic quants for superior accuracy and low memory use\n",
        "    \"unsloth/gemma-3-4b-it-unsloth-bnb-4bit\",\n",
        "    \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n",
        "    \"unsloth/gemma-3-27b-it-unsloth-bnb-4bit\",\n",
        "    # Qwen3 new models\n",
        "    \"unsloth/Qwen3-4B-unsloth-bnb-4bit\",\n",
        "    \"unsloth/Qwen3-8B-unsloth-bnb-4bit\",\n",
        "    # Other very popular models!\n",
        "    \"unsloth/Llama-3.1-8B\",\n",
        "    \"unsloth/Llama-3.2-3B\",\n",
        "    \"unsloth/Llama-3.3-70B\",\n",
        "    \"unsloth/mistral-7b-instruct-v0.3\",\n",
        "    \"unsloth/Phi-4\",\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/orpheus-3b-0.1-ft\",\n",
        "    max_seq_length= 2048, # Choose any for long context!\n",
        "    dtype = None, # Select None for auto detection\n",
        "    load_in_4bit = False, # Select True for 4bit which reduces memory usage\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXd9bTZd1aaL"
      },
      "source": [
        "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-22T00:51:37.365079Z",
          "iopub.status.busy": "2025-03-22T00:51:37.364731Z",
          "iopub.status.idle": "2025-03-22T00:51:44.221612Z",
          "shell.execute_reply": "2025-03-22T00:51:44.220949Z",
          "shell.execute_reply.started": "2025-03-22T00:51:37.365045Z"
        },
        "id": "6bZsfBuZDeCL",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 64, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 64,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vITh0KVJ10qX"
      },
      "source": [
        "<a name=\"Data\"></a>\n",
        "### Data Prep  \n",
        "\n",
        "We will use the `MrDragonFox/Elise`, which is designed for training TTS models. Ensure that your dataset follows the required format: **text, audio** for single-speaker models or **source, text, audio** for multi-speaker models. You can modify this section to accommodate your own dataset, but maintaining the correct structure is essential for optimal training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-22T00:51:44.222880Z",
          "iopub.status.busy": "2025-03-22T00:51:44.222617Z",
          "iopub.status.idle": "2025-03-22T00:52:16.516878Z",
          "shell.execute_reply": "2025-03-22T00:52:16.516033Z",
          "shell.execute_reply.started": "2025-03-22T00:51:44.222848Z"
        },
        "id": "LjY75GoYUCB8",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, concatenate_datasets, Audio\n",
        "\n",
        "# Keep only the fields we need + normalize gender to string\n",
        "def standardize(ds):\n",
        "    gfeat = ds.features.get(\"gender\", None)\n",
        "    names = getattr(gfeat, \"names\", None)  # ClassLabel -> names if present\n",
        "\n",
        "    def _norm(e):\n",
        "        g = e.get(\"gender\")\n",
        "        if isinstance(g, int) and names:\n",
        "            g = names[g]\n",
        "        e[\"gender\"] = (g if g is not None else \"unknown\")\n",
        "        return {\n",
        "            \"audio\": e[\"audio\"],\n",
        "            \"transcription\": e[\"transcription\"],\n",
        "            \"gender\": e[\"gender\"],\n",
        "        }\n",
        "\n",
        "    keep = {\"audio\", \"transcription\", \"gender\"}\n",
        "    return ds.map(_norm, remove_columns=[c for c in ds.column_names if c not in keep])\n",
        "\n",
        "# Load\n",
        "ds1 = load_dataset(\"Shekharmeena/shona_fleurs_filtered_dataset\", split=\"train\")\n",
        "ds2 = load_dataset(\"Beijuka/DigitalUmuganda_AfriVoice_shona\",         split=\"train\")\n",
        "\n",
        "# Standardize columns & types\n",
        "ds1 = standardize(ds1)\n",
        "ds2 = standardize(ds2)\n",
        "\n",
        "# (Recommended) cast both to a common decode sampling rate so Audio feature matches\n",
        "TARGET_SR = 48000\n",
        "ds1 = ds1.cast_column(\"audio\", Audio(sampling_rate=TARGET_SR))\n",
        "ds2 = ds2.cast_column(\"audio\", Audio(sampling_rate=TARGET_SR))\n",
        "\n",
        "# Concatenate + optional shuffle\n",
        "dataset = concatenate_datasets([ds1, ds2]).shuffle(seed=42)\n",
        "print(dataset)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mOgdzqcZmEq_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-22T00:52:16.518175Z",
          "iopub.status.busy": "2025-03-22T00:52:16.517841Z",
          "iopub.status.idle": "2025-03-22T00:52:35.039329Z",
          "shell.execute_reply": "2025-03-22T00:52:35.038356Z",
          "shell.execute_reply.started": "2025-03-22T00:52:16.518146Z"
        },
        "id": "zK94B-Pfioto",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#@title Tokenization Function\n",
        "\n",
        "import locale\n",
        "import torchaudio.transforms as T\n",
        "import os\n",
        "import torch\n",
        "from snac import SNAC\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "\n",
        "# ==== model / seq length cap (bump if your base LM supports more) ====\n",
        "MODEL_MAX_LEN = 2048\n",
        "\n",
        "# ---- SNAC encoder (24 kHz) ----\n",
        "snac_model = SNAC.from_pretrained(\"hubertsiuzdak/snac_24khz\").to(\"cuda\")\n",
        "\n",
        "def tokenise_audio(waveform, sr):\n",
        "  \"\"\"\n",
        "  waveform: numpy array (mono) from HF Audio feature\n",
        "  sr:       sampling_rate from the same example\n",
        "  \"\"\"\n",
        "  x = torch.from_numpy(waveform).unsqueeze(0).to(dtype=torch.float32)  # [1, T] CPU\n",
        "  x = T.Resample(orig_freq=sr, new_freq=24000)(x)                      # resample\n",
        "  x = x.unsqueeze(0).to(\"cuda\")                                       # [1,1,T] GPU\n",
        "\n",
        "  with torch.inference_mode():\n",
        "    codes = snac_model.encode(x)\n",
        "\n",
        "  # Interleave into 7 tokens per frame, then offset into audio token range\n",
        "  base = 128256 + 10  # = 128266 (your scheme)\n",
        "  all_codes = []\n",
        "  for i in range(codes[0].shape[1]):\n",
        "    all_codes.append(codes[0][0][i].item()        + base + 0*4096)\n",
        "    all_codes.append(codes[1][0][2*i].item()      + base + 1*4096)\n",
        "    all_codes.append(codes[2][0][4*i].item()      + base + 2*4096)\n",
        "    all_codes.append(codes[2][0][4*i + 1].item()  + base + 3*4096)\n",
        "    all_codes.append(codes[1][0][2*i + 1].item()  + base + 4*4096)\n",
        "    all_codes.append(codes[2][0][4*i + 2].item()  + base + 5*4096)\n",
        "    all_codes.append(codes[2][0][4*i + 3].item()  + base + 6*4096)\n",
        "  return all_codes\n",
        "\n",
        "def add_codes(example):\n",
        "    codes_list = None\n",
        "    try:\n",
        "        a = example.get(\"audio\")\n",
        "        if a and \"array\" in a:\n",
        "            sr = a.get(\"sampling_rate\", 48000)  # fallback if missing\n",
        "            codes_list = tokenise_audio(a[\"array\"], sr)\n",
        "    except Exception as e:\n",
        "        print(f\"Skipping row due to error: {e}\")\n",
        "    example[\"codes_list\"] = codes_list\n",
        "    return example\n",
        "\n",
        "dataset = dataset.map(add_codes, remove_columns=[\"audio\"])\n",
        "dataset = dataset.filter(lambda x: x[\"codes_list\"] is not None and len(x[\"codes_list\"]) > 0)\n",
        "\n",
        "# --- token id space (keep your original constants) ---\n",
        "tokeniser_length = 128256\n",
        "start_of_text = 128000\n",
        "end_of_text   = 128009\n",
        "\n",
        "start_of_speech = tokeniser_length + 1\n",
        "end_of_speech   = tokeniser_length + 2\n",
        "start_of_human  = tokeniser_length + 3\n",
        "end_of_human    = tokeniser_length + 4\n",
        "start_of_ai     = tokeniser_length + 5\n",
        "end_of_ai       = tokeniser_length + 6\n",
        "pad_token       = tokeniser_length + 7\n",
        "audio_tokens_start = tokeniser_length + 10  # = 128266\n",
        "\n",
        "def remove_duplicate_frames(example):\n",
        "    vals = example[\"codes_list\"]\n",
        "    if len(vals) % 7 != 0:\n",
        "        example[\"codes_list\"] = None\n",
        "        return example\n",
        "    result = vals[:7]\n",
        "    for i in range(7, len(vals), 7):\n",
        "        if vals[i] != result[-7]:\n",
        "            result.extend(vals[i:i+7])\n",
        "    example[\"codes_list\"] = result\n",
        "    return example\n",
        "\n",
        "dataset = dataset.map(remove_duplicate_frames)\n",
        "dataset = dataset.filter(lambda x: x[\"codes_list\"] is not None)\n",
        "\n",
        "tok_info = '''*** TEXT PROMPT FORMAT (with gender)\n",
        "Prompt = f\"{gender}: {transcription}\" if gender present, else just transcription.\n",
        "We also truncate to MODEL_MAX_LEN (keeping audio in multiples of 7 tokens).\n",
        "'''\n",
        "print(tok_info)\n",
        "\n",
        "# Resolve gender label names if it's a ClassLabel feature\n",
        "try:\n",
        "    gender_names = dataset.features[\"gender\"].names  # e.g., ['male', 'female']\n",
        "except Exception:\n",
        "    gender_names = None\n",
        "\n",
        "def create_input_ids(example):\n",
        "    # ----- gender prefix -----\n",
        "    g = example.get(\"gender\", None)\n",
        "    if isinstance(g, int) and gender_names:\n",
        "        gender_str = gender_names[g]\n",
        "    elif isinstance(g, str):\n",
        "        gender_str = g\n",
        "    else:\n",
        "        gender_str = None\n",
        "\n",
        "    # ----- text prompt -----\n",
        "    text_prompt = f\"{gender_str}: {example['transcription']}\" if gender_str else example[\"transcription\"]\n",
        "\n",
        "    # Tokenize text (expects `tokenizer` & `end_of_text` defined elsewhere)\n",
        "    text_ids = tokenizer.encode(text_prompt, add_special_tokens=True)\n",
        "    text_ids.append(end_of_text)\n",
        "\n",
        "    # ---- enforce max length BEFORE building final sequence ----\n",
        "    # layout: [s_human] + text_ids + [e_human, s_ai, s_speech] + codes + [e_speech, e_ai]\n",
        "    overhead = 1 + len(text_ids) + 1 + 1 + 1 + 2  # = len(text_ids) + 6\n",
        "    max_audio_tokens = MODEL_MAX_LEN - overhead\n",
        "    if max_audio_tokens < 7:\n",
        "        example[\"input_ids\"] = None\n",
        "        example[\"labels\"] = None\n",
        "        example[\"attention_mask\"] = None\n",
        "        return example\n",
        "\n",
        "    keep_audio = (max_audio_tokens // 7) * 7\n",
        "    audio_tokens = example[\"codes_list\"][:keep_audio]\n",
        "\n",
        "    input_ids = (\n",
        "        [start_of_human]\n",
        "        + text_ids\n",
        "        + [end_of_human]\n",
        "        + [start_of_ai, start_of_speech]\n",
        "        + audio_tokens\n",
        "        + [end_of_speech, end_of_ai]\n",
        "    )\n",
        "\n",
        "    if len(input_ids) > MODEL_MAX_LEN:\n",
        "        input_ids = input_ids[:MODEL_MAX_LEN]\n",
        "\n",
        "    example[\"input_ids\"] = input_ids\n",
        "    example[\"labels\"] = input_ids[:]\n",
        "    example[\"attention_mask\"] = [1] * len(input_ids)\n",
        "    return example\n",
        "\n",
        "# Build training columns\n",
        "dataset = dataset.map(create_input_ids, remove_columns=[\"transcription\", \"codes_list\"])\n",
        "dataset = dataset.filter(lambda x: x[\"input_ids\"] is not None and len(x[\"input_ids\"]) <= MODEL_MAX_LEN)\n",
        "\n",
        "columns_to_keep = [\"input_ids\", \"labels\", \"attention_mask\"]\n",
        "dataset = dataset.remove_columns([c for c in dataset.column_names if c not in columns_to_keep])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idAEIeSQ3xdS"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "Now let's use Huggingface  `Trainer`! More docs here: [Transformers docs](https://huggingface.co/docs/transformers/main_classes/trainer). We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`.\n",
        "\n",
        "**Note:** Using a per_device_train_batch_size >1 may lead to errors if multi-GPU setup to avoid issues, ensure CUDA_VISIBLE_DEVICES is set to a single GPU (e.g., CUDA_VISIBLE_DEVICES=0)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-22T00:34:09.688959Z",
          "iopub.status.busy": "2025-03-22T00:34:09.688649Z",
          "iopub.status.idle": "2025-03-22T00:34:09.729661Z",
          "shell.execute_reply": "2025-03-22T00:34:09.729001Z",
          "shell.execute_reply.started": "2025-03-22T00:34:09.688939Z"
        },
        "id": "95_Nn-89DhsL",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments,Trainer,DataCollatorForSeq2Seq\n",
        "trainer = Trainer(\n",
        "    model = model,\n",
        "    train_dataset = dataset,\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 1,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "         num_train_epochs = 1, # Set this for 1 full training run.\n",
        "        # max_steps = 60,\n",
        "        learning_rate = 2e-4,\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\", # Use this for WandB etc\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2ejIt2xSNKKp"
      },
      "outputs": [],
      "source": [
        "# @title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-22T00:34:12.049152Z",
          "iopub.status.busy": "2025-03-22T00:34:12.048862Z",
          "iopub.status.idle": "2025-03-22T00:34:14.404349Z",
          "shell.execute_reply": "2025-03-22T00:34:14.403239Z",
          "shell.execute_reply.started": "2025-03-22T00:34:12.049130Z"
        },
        "id": "yqxqAZ7KJ4oL",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "pCqnaKmlO1U9"
      },
      "outputs": [],
      "source": [
        "# @title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(\n",
        "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
        ")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekOmTR1hSNcr"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "Let's run the model! You can change the prompts\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apUdB40Ep6Ki"
      },
      "outputs": [],
      "source": [
        "prompts = [\n",
        "    \"Mhoro zita rangu ndinoitwa Scales,ndiri speech generation model yakagadzirwa kutaura semunhu\",\n",
        "]\n",
        "\n",
        "chosen_voice = None # None for single-speaker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {
          "iopub.execute_input": "2025-03-22T00:52:35.040842Z",
          "iopub.status.busy": "2025-03-22T00:52:35.040125Z",
          "iopub.status.idle": "2025-03-22T00:52:35.050560Z",
          "shell.execute_reply": "2025-03-22T00:52:35.049663Z",
          "shell.execute_reply.started": "2025-03-22T00:52:35.040818Z"
        },
        "id": "krYI8PrRJ6MX",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#@title Run Inference\n",
        "\n",
        "\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "# Moving snac_model cuda to cpu\n",
        "snac_model.to(\"cpu\")\n",
        "\n",
        "prompts_ = [(f\"{chosen_voice}: \" + p) if chosen_voice else p for p in prompts]\n",
        "\n",
        "all_input_ids = []\n",
        "\n",
        "for prompt in prompts_:\n",
        "  input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "  all_input_ids.append(input_ids)\n",
        "\n",
        "start_token = torch.tensor([[ 128259]], dtype=torch.int64) # Start of human\n",
        "end_tokens = torch.tensor([[128009, 128260]], dtype=torch.int64) # End of text, End of human\n",
        "\n",
        "all_modified_input_ids = []\n",
        "for input_ids in all_input_ids:\n",
        "  modified_input_ids = torch.cat([start_token, input_ids, end_tokens], dim=1) # SOH SOT Text EOT EOH\n",
        "  all_modified_input_ids.append(modified_input_ids)\n",
        "\n",
        "all_padded_tensors = []\n",
        "all_attention_masks = []\n",
        "max_length = max([modified_input_ids.shape[1] for modified_input_ids in all_modified_input_ids])\n",
        "for modified_input_ids in all_modified_input_ids:\n",
        "  padding = max_length - modified_input_ids.shape[1]\n",
        "  padded_tensor = torch.cat([torch.full((1, padding), 128263, dtype=torch.int64), modified_input_ids], dim=1)\n",
        "  attention_mask = torch.cat([torch.zeros((1, padding), dtype=torch.int64), torch.ones((1, modified_input_ids.shape[1]), dtype=torch.int64)], dim=1)\n",
        "  all_padded_tensors.append(padded_tensor)\n",
        "  all_attention_masks.append(attention_mask)\n",
        "\n",
        "all_padded_tensors = torch.cat(all_padded_tensors, dim=0)\n",
        "all_attention_masks = torch.cat(all_attention_masks, dim=0)\n",
        "\n",
        "input_ids = all_padded_tensors.to(\"cuda\")\n",
        "attention_mask = all_attention_masks.to(\"cuda\")\n",
        "generated_ids = model.generate(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask,\n",
        "      max_new_tokens=1200,\n",
        "      do_sample=True,\n",
        "      temperature=0.6,\n",
        "      top_p=0.95,\n",
        "      repetition_penalty=1.1,\n",
        "      num_return_sequences=1,\n",
        "      eos_token_id=128258,\n",
        "     use_cache = True\n",
        "  )\n",
        "token_to_find = 128257\n",
        "token_to_remove = 128258\n",
        "\n",
        "token_indices = (generated_ids == token_to_find).nonzero(as_tuple=True)\n",
        "\n",
        "if len(token_indices[1]) > 0:\n",
        "    last_occurrence_idx = token_indices[1][-1].item()\n",
        "    cropped_tensor = generated_ids[:, last_occurrence_idx+1:]\n",
        "else:\n",
        "    cropped_tensor = generated_ids\n",
        "\n",
        "mask = cropped_tensor != token_to_remove\n",
        "\n",
        "processed_rows = []\n",
        "\n",
        "for row in cropped_tensor:\n",
        "    masked_row = row[row != token_to_remove]\n",
        "    processed_rows.append(masked_row)\n",
        "\n",
        "code_lists = []\n",
        "\n",
        "for row in processed_rows:\n",
        "    row_length = row.size(0)\n",
        "    new_length = (row_length // 7) * 7\n",
        "    trimmed_row = row[:new_length]\n",
        "    trimmed_row = [t - 128266 for t in trimmed_row]\n",
        "    code_lists.append(trimmed_row)\n",
        "\n",
        "\n",
        "def redistribute_codes(code_list):\n",
        "  layer_1 = []\n",
        "  layer_2 = []\n",
        "  layer_3 = []\n",
        "  for i in range((len(code_list)+1)//7):\n",
        "    layer_1.append(code_list[7*i])\n",
        "    layer_2.append(code_list[7*i+1]-4096)\n",
        "    layer_3.append(code_list[7*i+2]-(2*4096))\n",
        "    layer_3.append(code_list[7*i+3]-(3*4096))\n",
        "    layer_2.append(code_list[7*i+4]-(4*4096))\n",
        "    layer_3.append(code_list[7*i+5]-(5*4096))\n",
        "    layer_3.append(code_list[7*i+6]-(6*4096))\n",
        "  codes = [torch.tensor(layer_1).unsqueeze(0),\n",
        "         torch.tensor(layer_2).unsqueeze(0),\n",
        "         torch.tensor(layer_3).unsqueeze(0)]\n",
        "\n",
        "  # codes = [c.to(\"cuda\") for c in codes]\n",
        "  audio_hat = snac_model.decode(codes)\n",
        "  return audio_hat\n",
        "\n",
        "my_samples = []\n",
        "for code_list in code_lists:\n",
        "  samples = redistribute_codes(code_list)\n",
        "  my_samples.append(samples)\n",
        "from IPython.display import display, Audio\n",
        "if len(prompts) != len(my_samples):\n",
        "  raise Exception(\"Number of prompts and samples do not match\")\n",
        "else:\n",
        "  for i in range(len(my_samples)):\n",
        "    print(prompts[i])\n",
        "    samples = my_samples[i]\n",
        "    display(Audio(samples.detach().squeeze().to(\"cpu\").numpy(), rate=24000))\n",
        "# Clean up to save RAM\n",
        "del my_samples,samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMuVrWbjAzhc"
      },
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Saving, loading finetuned models\n",
        "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
        "\n",
        "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upcOlWe7A1vc"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"lora_model\")  # Local saving\n",
        "tokenizer.save_pretrained(\"lora_model\")\n",
        "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
        "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f422JgM9sdVT"
      },
      "source": [
        "### Saving to float16\n",
        "\n",
        "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHjt_SMYsd3P"
      },
      "outputs": [],
      "source": [
        "# Merge to 16bit\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
        "\n",
        "# Merge to 4bit\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
        "\n",
        "# Just LoRA adapters\n",
        "if False:\n",
        "    model.save_pretrained(\"model\")\n",
        "    tokenizer.save_pretrained(\"model\")\n",
        "if False:\n",
        "    model.push_to_hub(\"hf/model\", token = \"\")\n",
        "    tokenizer.push_to_hub(\"hf/model\", token = \"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jj2hq2BiDrw"
      },
      "source": [
        "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
        "\n",
        "Some other links:\n",
        "1. Train your own reasoning model - Llama GRPO notebook [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb)\n",
        "2. Saving finetunes to Ollama. [Free notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)\n",
        "3. Llama 3.2 Vision finetuning - Radiography use case. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)\n",
        "6. See notebooks for DPO, ORPO, Continued pretraining, conversational finetuning and more on our [documentation](https://docs.unsloth.ai/get-started/unsloth-notebooks)!\n",
        "\n",
        "<div class=\"align-center\">\n",
        "  <a href=\"https://unsloth.ai\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "  <a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n",
        "  <a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a>\n",
        "\n",
        "  Join Discord if you need help + ⭐️ <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ⭐️\n",
        "</div>\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30919,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}